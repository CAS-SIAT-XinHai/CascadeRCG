{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangdi/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All knowledge database gets ready !\n"
     ]
    }
   ],
   "source": [
    "def vectors_db(embed_path, db_path):\n",
    "    query_instruction = \"为这个句子生成表示以用于检索相关文章：\"\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name= embed_path,\n",
    "                                            model_kwargs={'device': 'cuda'},\n",
    "                                            query_instruction=query_instruction) \n",
    "    vectordb = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "    return vectordb\n",
    "import random\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "import argparse\n",
    "from FlagEmbedding import FlagModel\n",
    "from sklearn.cluster import KMeans\n",
    "import logging\n",
    "EMBED_PATH = \"/data/yangdi/bge-large-zh-v1.5\"\n",
    "ALL_DB_PATH = \"/data/yangdi/ALL-bge-1.5-300\"\n",
    "all_vectordb = vectors_db(EMBED_PATH, ALL_DB_PATH)\n",
    "print('All knowledge database gets ready !')\n",
    "\n",
    "with open(\"/data/yangdi/data/psyqa_test.json\", encoding='utf-8') as f:\n",
    "    datas = json.load(f)\n",
    "# QWEN_PATH = \"/data/yangdi/qwen/Qwen1___5-14B-Chat\"\n",
    "# query_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     QWEN_PATH,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(QWEN_PATH)\n",
    "RERANKER_PATH = '/data/pretrained_models/maidalun/bce-reranker-base_v1'\n",
    "model = CrossEncoder(RERANKER_PATH, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reranker_top2(sentence_pairs):\n",
    "# calculate scores of sentence pairs\n",
    "    sort_scores = {}\n",
    "    scores = model.predict(sentence_pairs)\n",
    "    for i, score in enumerate(scores):\n",
    "        sort_scores[i] = score\n",
    "    scores_ = sorted(sort_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    index1, index2 = scores_[0][0], scores_[1][0]\n",
    "    return index1, index2\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from dashscope import Generation\n",
    "from http import HTTPStatus\n",
    "from dashscope import Generation\n",
    "from dashscope.api_entities.dashscope_response import Role\n",
    "import dashscope\n",
    "dashscope.api_key=\"sk-9f54f947734647c89c7d1e37a8054c41\"\n",
    "def query_Qwen_7B(prompt, stop_=None):\n",
    "    # Instead of using model.chat(), we directly use model.generate()\n",
    "    # But you need to use tokenizer.apply_chat_template() to format your inputs as shown below\n",
    "    device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "    # # Now you do not need to add \"trust_remote_code=True\"\n",
    "    # query_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     QWEN_PATH,\n",
    "    #     torch_dtype=\"auto\",\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(QWEN_PATH)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a psychological expert and specialized in psychological knowledge.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Directly use generate() and tokenizer.decode() to get the output.\n",
    "    # Use `max_new_tokens` to control the maximum output length.\n",
    "    response = Generation.call(\n",
    "        'qwen1.5-14b-chat',\n",
    "        messages=messages,\n",
    "        stop=stop_\n",
    "    )\n",
    "    if response.status_code == HTTPStatus.OK:\n",
    "        response = response.output[\"text\"]\n",
    "        print(response)\n",
    "    else:\n",
    "        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "            response.request_id, response.status_code,\n",
    "            response.code, response.message\n",
    "        ))\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    prompt = f\"\"\"\n",
    "    根据下面知识回答问题：\n",
    "    问题：{question}\n",
    "    知识：{knowledge}\n",
    "    \"\"\"\n",
    "    query = model_type('Qwen')\n",
    "    res = query(prompt)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2376f610b407cb12a32bf968ff824b0be5439655027d2ddf6e80e1d1125ab0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
